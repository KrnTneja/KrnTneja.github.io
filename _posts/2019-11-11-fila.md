---
title:  "Basics of Reinforcement Learning"
date:   2019-11-11
permalink: /posts/2019/fila-summary
tags:
  - reinforcement-learning
  - multi-armed-bandits
  - markov-decision-processes
  - concentration-inequalities
---

I took the course <i>Foundations of Intelligent and Learning Agents</i> in Autumn 2019 offered by [Prof. Shivaram Kalyanakrishnan](https://www.cse.iitb.ac.in/~shivaram/) at IIT Bombay. I found the course content very exciting and decided to share some basic intuition and ideas from reinforcement learning in this blog post. 

Consider an intelligent agent $M$ in an environment $E$ where it can take possible actions $A = \{a_1, \cdots, a_n\}$. At each discrete time step $t$, the agent will get a reward $r_t$ and it will move in the state space from its previous state $s_t$ to $s_{t+1}$. The objective of the agent is to maximize its cumulative reward in the long run i.e. $\sum_{t=1}^{\infty} r_t$. 

To do this agent needs to do two things:
1. <b>Exploration</b>: Explore $E$ to ensure that it has a good model of the environment it is in by taking different actions it different states.
2. <b>Sequential Decision Making</b>: Take its previous states into account to take actions based on the model of $E$ that it has formed. 

Reinforcement learning is the combination of exploration and sequential decision making. We study it in three steps:
1. <b>Multi-armed Bandits (MABs)</b>: the idea of exploration and exploitation (without accounting for states)
2. <b>Markov Decision Processes (MDPs)</b>: the idea of states and transitions between states
3. <b>Reinforcement Learning (RL)</b>: combining and two ideas and designing algorithms that explore and learn the environment while accounting for previous states

<h2>Multi-armed Bandits</h2>
Consider 3 coins with unknown biases (the probabilities of getting heads). You are an agent who has to toss a coin once every minute for next 100 minutes. Your objective is to maximize the number of heads. What will you do? Ideally you would want to keep tossing the coin with maximum probability of heads but the biases are not known and need to be estimated. So you need to <i>explore</i> the coins to estimate probabilities and then choose the most promising coins. One way would be to toss each coin 10 times, calculate the estimates of probabilities and then keep tossing the coin for remaining 70 time steps. But, is it the optimal way? 

Before answering this, we'll formalize the problem. Let the MAB have $n$ arms ($n=3$ coins in above examples) where possible actions $$A = \{ a_1, \cdots, a_n \}$$ correspond to pulling the respective arms. We will keep the problem simple by using only Bernoulli rewards. Mean reward of $a_i$ is $p_i$ and $T$ is the total number of times steps (also called horizon or budget). Let $a^\* $ denote the optimal arm and $p^\*$ be the corresponding reward. 

We define the notion of regret to measure of optimality of algorithm. Cumulative regret at time step $T$ is defined as 
$$R(T) = \sum_{t=1}^T [p^* - \mathbb{E}(r_t)]$$

where $r_t$ is the reward from arm pulled at time t. Our objective will be to minimize the regret when we evaluate the algorithms. The naive algorithm described above is not the best known algorithm in terms of regret. In particular, it has a cumulative regret $O(T)$ because there is a constant probability with which it won't be sampling $a^*$ after the initial exploration. Inituitively, the best known algorithms do the following two things:
1. They will never stop exploring and keep sampling all the arms with some probability to allow the optimal arm to eventually outperform the remaining arms in terms of probability estimates.
2. They will lower the number of times the non-optimal arms are sampled over time, with zero probability of sampling them in limit.

Here are three such algorithms which I will not discuss in detail here but I will mention that they all have $\log(T)$ regret.
1. <b>Upper Confidence Bound (UCB) algorithm</b>: This algorithm chooses the arms based on an optimistic estimate of the average rewards which is given by
$$u_i = \hat{p}_i + \sqrt{\frac{2 \ln(T)}{T_i}}$$
where $\hat{p}_i$ is the MLE estimate and $T_i$ is number of times $a_i$ has been sampled till time $T$.
2. <b>Kullback-Leibler UCB (KL-UCB) algorithm</b>: Very similar to UCB except that the estimate is obtained by upper bounding the KL-divergence between Bernoulli($\hat{p}_i$) and Bernoulli($u_i$).
3. <b>Thompson Sampling algorithm</b>: The reward estimate is obtained by sampling from a posterior distribution for each individual arm and the one with highest reward estimate is pulled.

<h2>Markov Decision Processes</h2>
An MDP is defined by its five parameters:
1. $S$, the set of states that agent can be in
2. $A$, the set of actions that agent can take
3. $T : S \times A \rightarrow \mathcal{P}_S$, the transition function defined on each <code>(state, action)</code> pair gives the probability distribution of next state over $S$
4. $R : S \times A \times S \rightarrow \mathbb{R}$, the reward function defined for each <code>(state, action, next state)</code> triplet
5. $\gamma$, the discount factor

Note that $$ \mathcal{P}_S $$,
is the set of all probability distributions over $S$.
The reward function can be fixed as above or stochastic where the mean reward can be expressed as above.
Since the sum $ \sum_{t=1}^T r_t $ can blow up as $ T \rightarrow \infty $,
we discount the future rewards by using the discounted cumulative reward $ \sum_{t=1}^T \gamma^{t-1} r_t$.

A determistic policy followed by an agent is a mapping $$\pi : S \rightarrow A \$$. If the agent is following a stochastic policy, mapping will be into the space of probability distributions over $A$ i.e. $$\pi : S \rightarrow \mathcal{P}_A$$. If the agent is in state $s^t$ at time $t$ it will choose action by using $a^t \sim \pi(s^t)$, and go to the next state decided by $$ s^{t+1} \sim T(s^t, a^t) $$ and recieve a reward given by $$ r^t \sim R(s^t, a^t, s^{t+1})$$ (assuming the most general case of policy, transitions and rewards being stochastic).

When an agent follows a policy in an environment, the path it follows it called a <i>trajectory</i> which is given by the sequence $$\{s^0 a^0 r^0 s^1 a^1 r^1 \cdots \}$$. A <i>value function</i> for a policy $\pi$, written as $$ V^\pi : S \rightarrow \mathbb{R} $$ is a mapping from state to the discounted cumulative reward that agent will receive if it keeps following the policy $\pi$. 

A policy $$ \pi^* $$ is an optimal policy if it satisfies 

$$\pi^* = \arg \max_\pi V^\pi(s_i)\text{ }\forall i.$$
  
But does every MDP has an optimal policy? I will not go into the proof here but the answer is, wait for it, a <i>yes</i>.

Mathematically, we define the value function as 

$$V^\pi (s) = \mathbb{E}_\pi \Big[ \sum_{t=0}^{\infty} \gamma^t r^t |  s^0 = s, a^i = \pi (s^i) \forall i\Big].$$

For a determistic policy $\pi$, it is not hard to show that above equation can be written recursively in terms of next state $s'$ as 

$$V^\pi(s) = \sum_{s'\in S} T(s, \pi(s), s') [R(s, \pi(s), s') + \gamma V^\pi(s')].$$

This equation is called the Bellman's equation. If we know all the paramters of MDP and a policy, we can write this as a set of 
$$|S|$$ 
linear equations which can be solved to get $V^\pi(s)$.  This process is called <i>policy evaluation</i>.

Another important function that we come across often is the <i>action value function<i>, denoted by $$ Q^\pi : S \times A \rightarrow \mathbb{R}$$, which is given by

$$ Q^\pi (s, a) = \sum_{s' \in S} T(s, \pi(s), s') [ R(s, \pi(s), s') + \gamma V^\pi(s') ].$$

Note that $Q^\pi(s, \pi(s)) = V^\pi(s)$. 

Since we know how to evaluate policies, we can now iterate over all policies and check which policy satisfies the condition of optimality to find the optimal policy. But the main problem with this method is that number of policies is exponential in number of states. There are better methods to find optimal policy which we shall discuss next. Imagine you knew that $$\pi^*$$ is the optimal policy, so you could calculate the action value function $$Q^*(s,a)$$. To get the optimal action from here, you could use $$\pi^*(s) = \arg \max_a Q^\pi(s,a)$$. In fact, we can also write that $$V^*(s) = \max_a Q^*(s, a)$$ which expands to 

$$ V^*(s) = \max_a \sum_{s' \in S} T(s, \pi^*(s), s') [ R(s, \pi^*(s), s') + \gamma V^*(s') ].$$


These set of 
$$ | S | $$
equations are called <i>Bellman's optimality equations</i>. Solving for optimal policy is a matter of solving these 
$$|S|$$
equations. There are three main algorithms to solve these equations which I'll briefly describe below.

<h3>Value Iteration</h3>

Start from some value initialization of value function $V_0$. Keep using the equation below while using the MDP until convergence.  

$$ V_{t+1}(s) = \max_{a \in A} \sum_{s' \in S} T(s, a, s') [ R(s, a, s') + \gamma V^*(s') ] $$  

It can be shown using a contraction argument that $V_t$ converges to $$V^*$$ in limit.

<h3>Linear Programming</h3>

It can be easily shown that Bellman's optimality equations are equivalent to following LP problem which can be solved using any known LP solver.  

$$\min \sum_{s \in S} V(s) $$

$$\text{ subject to } V(s) \geq \sum_{s' \in S} T(s, a, s') [ R(s, a, s') + \gamma V(s') ], \forall s \in S, \forall a \in A$$.

<h3>Policy Iteration</h3>

We initialize the algorithm with a random policy $\pi_0$. A state is called <i>improvable</i> under policy $\pi$ if $\exists a \in A, a \neq \pi(s)$ such that $Q^\pi(s, a) > Q^\pi(s, \pi(s))$. An action satisfying these conditions is called an <i>improved action for state $s$</i>. 

At each iteration, for each state, we check if the state $s$ is improvable under current policy $\pi_t$ and then improve the policy to an take an improved action $\pi_{t+1}(s)$ until there are no improvable states. It can be shown that this algorithm strictly improves the value function at each step and converges to the optimal policy in a finite number of steps (since the number of policies are finite). 

<h2>Reinforcement Learning</h2>
In the discussion of MDPs, conveniently, all the paramters of MDP were assumed to be known. In reinforcement learning, however, we don't have an access to these paramters. One obvious way to go around this problem is to estimate the paramters by getting <i>experience</i> in the environment. We can maintain two 3-dimensional matrices, $\hat{T}$ for estimated state transition probability and $\hat{R}$ for estimated mean rewards. 

$$\hat{T}(s, a, s') = \frac{\sum_t \mathbb{1}_{s^t=s,a^t=a,s^{t+1}=s'}}{\sum_t \mathbb{1}_{s^t=s,a^t=a}} $$

$$\hat{R}(s, a, s') = \frac{\sum_t r^t \mathbb{1}_{s^t=s,a^t=a,s^{t+1}=s'}}{\sum_t  \mathbb{1}_{s^t=s,a^t=a,s^{t+1}=s'}} $$

Such algorithms which keep an estimate of the model of size complexity 
$$O(|S|^2 |A|)$$
are called model-based algorithms. On the contrary, we have model-free algorithms which have lower size complexity as we will see later.

<h3>Monte Carlo Method</h3>
By doing multiple iterations in the MDP, we can retrieve the information we need to choose our policy. This retrieval is called <i>estimation</i>. Approximating optimal policy using this information is called <i>control</i>. Monte Carlo estimation estimates $Q^\pi(s,a)$ given a policy $\pi$. To use Monte Carlo estimation for control, we need to alternate

<b>Also see:</b>
* Simple Regret
* Gaussian Reward MAB
* Adversarial Bandits
* Non-stationary Bandits
* Pure Exploration Problems
* Proof of why every MDP has an optimal policy
* Monte Carlo Methods: First-visit and every-visit

<b>Important Papers:</b>
* Asymptotically efficient adaptive allocation rules: Showed that MABs regret can't be better than log(T)

<!-- Attention-based models belong to a class of models commonly called sequence-to-sequence models. The aim of these models, as name suggests, it to produce an output sequence given an input sequence which are, in general, of different lengths. Let input sequence be $\mathbf{x} = \\{x_1, x_2, \ldots, x_T\\}$ and output sequence be $\mathbf{y} = \\{y_1, y_2, \ldots, y_U\\}$. There are 2 parts of a sequence-to-sequence models viz. encoder and decoder, both of which are RNNs. Encoder encodes the given input producing a series of hidden states $\mathbf{h} = \\{h_1, h_2, \ldots, h_T\\}$ of input $\mathbf{x}$.

&&\mathbf{h} = Encoder(\mathbf{x})&&

![](/images/encoder_new.png)
*Bi-directional RNN based encoder takes the input sequence $\mathbf{x} = \\{x_1, x_2, \ldots, x_T\\}$ outputs the hidden states $\mathbf{h} = \\{h_1, h_2, \ldots, h_T\\}$ (memory vectors).*

<h2>References</h2>

<b id='enc-dec'>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</b><br>
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio <br>
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)

<b id='2lstm'>Sequence to Sequence Learning with Neural Networks</b><br>
Ilya Sutskever, Oriol Vinyals, Quoc V. Le <br>
Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS 2014)

<b id='ctc'>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</b><br>
Alex Graves, Santiago Fer<e>&aacute;</e>nandez, Faustino Gomez, J<e>&uuml;</e>rgen Schmidhuber <br>
Proceedings of the 23rd International Machine Learning Conference, 2006

<b id='soft'>Neural Machine Translation by Jointly Learning to Align and Translate</b><br>
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio <br>
International Conference on Learning Representations, 2015

<b id='luong'>Effective Approaches to Attention-based Neural Machine Translation</b><br>
Minh-Thang Luong, Hieu Pham, Christopher D. Manning
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing  

<h2>Further Reading</h2>

<b>Structured Attention Networks</b> <br>
Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush <br> 
5th International Conference on Learning Representations, 2017

<b>Listen, Attend and Spell</b> <br>
William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals <br>
2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)

<b>Monotonic Chunkwise Attention</b><br>
Chung-Cheng Chiu, Colin Raffel <br>
International Conference on Learning Representations, 2018

<b>Attention-Based Models for Speech Recognition</b><br>
Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, Yoshua Bengio<br>
Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS 2015)

<b>Online and Linear-Time Attention by Enforcing Monotonic Alignments</b><br>
Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck <br>
Proceedings of the 34th International Conference on Machine Learning, 2017

<b>Attention Is All You Need</b><br>
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin <br>
31st Conference on Neural Information Processing Systems (NIPS 2017)

<b>State-of-the-art Speech Recognition With Sequence-to-Sequence Models</b><br>
Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani <br>
arXiv:1712.01769

<h2> Addtional Notes </h2>

1. In section 2.3.1, the architecture uses 4-layered LSTM model for both input-sequence reading and generating output-sequence. Long-range dependencies refer to the temporal dependencies in sequences and not the forward pass of the model.

2. In section 2.3.1, suppose an input sequence $a-b-c-d$ has to be translated to output sequence $A-B-C-D$. If input LSTM model sees $d-c-b-a$ instead of $a-b-c-d$, the output \'summary\' vector is influenced most by $a$ and hence it becomes easier to realize for output LSTM to start with $A$. Although average dependency length between the pairs ($a-b-c-d$, $A-B-C-D$) and ($d-c-b-a$, $A-B-C-D$) is same, the model probably finds it easier to start the sequence with initial $A$ and $B$ and so on. Temporal dependencies later help in generating the $C$ and $D$ more easily on average. This is very crude reasoning and there is no \'proof\' as such but this phenomenon of better performance on the input sequence reversal is observed and can be explained as above. -->